
experiment:
    alias: "CustomExp-1"
    logger: "process.txt"
env:
    name: "FetchReach-v1"
ddpg:
    extractor: [256, 256, 256]      # number of neurons in each hidden layer
    arch: "mlp"
    activation: "relu"
    pi_lr: 0.001                    # actor learning rate
    q_lr: 0.001                     # critic learning rate
    replay_size: 100000             # for experience replay
    polyak: 0.95                    # polyak averaging coefficient
    action_l2: 1.0                  # quadratic penalty on actions
    clip_obs: 200. 
    gamma: 0.95                     # discount factor
    clip_return: True               # whether or not returns should be clipped     
training:     
    cycles: 50                      # per epoch
    updates: 40                     # training batches per cycle
    batch_size: 128                 # per mpi thread
    test_rollouts: 10               # number of test rollouts per epoch
    demo_episodes: 20               # number of episodes for demonstration
exploration:      
    random_eps: 0.3                 # percentage of time a random action is taken
    noise_eps: 0.2                  # std of gaussian noise added to not-completely-random actions
her:
    sampling_strategy: "future"     # supported modes: future
    replay_k: 4                     # number of additional goals used for replay
normalization:
    norm_clip: 5                    # normalized observations are cropped to this values
auxiliary:      
    checkpoint_freq: 5              # frequency for saving the framework state in epochs
    seed: 0                         # seed for any Random Number Generators
    checkpoint_dir: "data/FetchReach-v1/model"
    device: "cuda:0"
    elite_metric: "success"
    auto_save: True
